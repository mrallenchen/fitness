{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pandas and postgres to work together\n",
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "\n",
    "# We are also going to do some basic viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/df_full.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11826"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:,4:9]\n",
    "y_train = df_train['category']\n",
    "X_test = df_test.iloc[:,4:9]\n",
    "y_test = df_test['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9460"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that histogram of this indicates that there is an imbalanced problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn.over_sampling\n",
    "\n",
    "# setup for the ratio argument of RandomOverSampler initialization\n",
    "Obese = np.sum(y_train == 'Obese')\n",
    "Average = np.sum(y_train == 'Average')\n",
    "Fitness = np.sum(y_train == 'Fitness')\n",
    "Athlete = np.sum(y_train == 'Athlete')\n",
    "ratio = {'Obese': Obese, 'Average': Average*2, 'Fitness': Fitness*5, 'Athlete': Athlete*10} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code for consideration for oversampling\n",
    "# randomly oversample positive samples:\n",
    "ROS = imblearn.over_sampling.RandomOverSampler(sampling_strategy = ratio, random_state=42) \n",
    "    \n",
    "# X_tr_rs, y_tr_rs = ROS.fit_resample(df_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions (Model Selection Pipeline)\n",
    "Things to address in pipeline:\n",
    "\n",
    "Class Imbalance (sampling metrics)\\\n",
    "Kfold cross validation (small dataset) \\\n",
    "Modeling (parameter tuning, class weights for those it applies to) \\\n",
    "Metrics selected above (F1) \\\n",
    "ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from operator import itemgetter\n",
    "from sklearn import tree\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum = DummyClassifier(strategy='prior')\n",
    "dum_dum = dum.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_dum.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y_train, dum_dum.predict(y_train), average = 'macro')\n",
    "#note that micro will simply calculate true positives / (TP+FP), in multiclass this equals TP/all guesses = 74%\n",
    "#macro will calculate TP/(TP+FP) for each class, then average across. In naive case, this is 74%*0.25 = 0.185 because obese is one of 4 classes \n",
    "#weighted this is 74%* 74% (precision for obese*obese percentage) + 0%*19% (percentage average) + 0*5%(percent fit) +0*2% (percent athlete) because obese is weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.precision_score(y_train, dum_dum.predict(X_train), average = 'macro', zero_division = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process\n",
    "#For each algorithm - Decision Tree, KNeighbors, Logistic Regression, XGBoost -:\n",
    "    \n",
    "#    Find the best parameters for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scorer = make_scorer(f1_score,average='macro',zero_division=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle = True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _BaseKFold.split at 0x7f975b79f0b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.split(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTCparameters = {'max_depth':range(2,11)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 5531)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (3954) in class Average will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (2440) in class Fitness will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 5532)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 1382)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (3954) in class Average will be larger than the number of samples in the majority class (class #Obese -> 1382)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (2440) in class Fitness will be larger than the number of samples in the majority class (class #Obese -> 1382)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 5531)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (3954) in class Average will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (2440) in class Fitness will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 5531)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (3954) in class Average will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (2440) in class Fitness will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 5531)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (6914) in class Obese will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (3954) in class Average will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n",
      "/Users/allen/opt/anaconda3/envs/metis/lib/python3.8/site-packages/imblearn/utils/_validation.py:318: UserWarning: After over-sampling, the number of samples (2440) in class Fitness will be larger than the number of samples in the majority class (class #Obese -> 1383)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DecisionTreeClassifier(max_depth=10),\n",
       " 0.2573305586457212,\n",
       " {'max_depth': 10},\n",
       " array([0.2111274 , 0.2240519 , 0.23459968, 0.24601399, 0.27530689,\n",
       "        0.30488288, 0.35595166, 0.40459926, 0.44712441]),\n",
       " array([0.21112739, 0.22175882, 0.22501979, 0.22865624, 0.23206115,\n",
       "        0.24102499, 0.25215568, 0.2563504 , 0.25733056]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SamplingGridSearchCV(ROS, DecisionTreeClassifier(), X_train, y_train, DTCparameters, folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create KFOLD GRIDSEARCH WITH SAMPLING TECHNIQUES.\n",
    "#return best model based on F1 score, average = \"macro\"\n",
    "\n",
    "\n",
    "#helper function\n",
    "def model_scores(scores):\n",
    "    '''\n",
    "    Input: Scores within a list (of folds) containing lists (of scores by models with diff params)\n",
    "    Output: (1) array with all scores, (2) array with average scores for each fold\n",
    "    '''\n",
    "    flat_list = []\n",
    "    for fold in scores:\n",
    "        for model in fold:\n",
    "            flat_list.append(model)\n",
    "    X = np.array(flat_list).reshape(len(scores),len(scores[0]))\n",
    "    return X, X.mean(axis = 0)\n",
    "    \n",
    "#Main function    \n",
    "def SamplingGridSearchCV(resampler, model, X_train, y_train, param_grid, folds=5):\n",
    "        \n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle = True, random_state=42)\n",
    "\n",
    "    # Lists to hold results\n",
    "    parameters = []\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "\n",
    "    # Get indices for split\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    #create folds with resampling\n",
    "    for indices in kf.split(X_train, y_train):\n",
    "        train_ind = indices[0]\n",
    "        val_ind = indices[1]\n",
    "        X_tr, y_tr = X_train[train_ind], y_train[train_ind]\n",
    "        X_resampled_train, y_resampled_train = resampler.fit_sample(X_tr,y_tr)\n",
    "        X_val, y_val = X_train[val_ind], y_train[val_ind]\n",
    "        X_resampled_val, y_resampled_val = resampler.fit_sample(X_val, y_val)\n",
    "        \n",
    "        \n",
    "        # initialize lists to hold results for each model/parameter\n",
    "        fold_parameters = []\n",
    "        fold_train_scores = []\n",
    "        fold_val_scores = []\n",
    "        \n",
    "        #fit model with each parameter and append results to list for current fold\n",
    "        for g in ParameterGrid(param_grid):\n",
    "            model.set_params(**g)\n",
    "            mod = model.fit(X_tr, y_tr)    \n",
    "            train_score = f1_score(y_tr, model.predict(X_tr), average='macro', zero_division = 0)\n",
    "            val_score = f1_score(y_val, model.predict(X_val), average='macro', zero_division = 0)\n",
    "            \n",
    "            #add results to list \n",
    "            fold_parameters.append(g)\n",
    "            fold_train_scores.append(train_score)\n",
    "            fold_val_scores.append(val_score)\n",
    "        \n",
    "        #append results from the fold into larger list\n",
    "        parameters.append(fold_parameters)\n",
    "        train_scores.append(fold_train_scores)\n",
    "        val_scores.append(fold_val_scores)\n",
    "\n",
    "    #aggregate all results and prepare outputs\n",
    "    parameters = parameters[0]\n",
    "    \n",
    "    all_train_scores, model_train_scores = model_scores(train_scores)\n",
    "    all_val_scores, model_val_scores = model_scores(val_scores)\n",
    "    best_params = parameters[max(enumerate(model_val_scores), key=itemgetter(1))[0]]\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model_score = max(model_val_scores)\n",
    "   \n",
    "    return best_model, best_model_score, best_params, model_train_scores, model_val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SamplingGridSearchCV(resampler, model, X_train, y_train, param_grid, folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run loop on multiple codes\n",
    "\n",
    "#get values and create model v model plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to obtain results and plots (based on val data) for selected model with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier(class_weight= {'Obese':1,'Average':2,'Fitness':5,'Athlete':10})\n",
    "parameters = {'max_depth':range(2,11)}\n",
    "DTC_mod_CV = GridSearchCV(DTC, parameters,scoring=my_scorer)\n",
    "DTC_fitted = DTC_mod_CV.fit(X_train,y_train)a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5}\n",
      "0.3149262913774683\n"
     ]
    }
   ],
   "source": [
    "print(DTC_fitted.best_params_)\n",
    "print(DTC_fitted.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = metrics.confusion_matrix(y_train, DTC_fitted.predict(X_train),labels=['Athlete','Fitness','Average','Obese'])\n",
    "conf = pd.DataFrame(conf, columns=['Athlete','Fitness','Average','Obese'], index = ['Athlete','Fitness','Average','Obese'])\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(test_conf, annot = True, cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_conf = metrics.confusion_matrix(y_test, dum_dum.predict(X_test),labels=['Athlete','Average','Fitness','Obese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':range(2,11)}\n",
    "KNN_mod_CV = GridSearchCV(KNN, parameters,scoring=my_scorer)\n",
    "KNN_fitted = KNN_mod_CV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KNN_fitted.best_params_)\n",
    "print(KNN_fitted.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = metrics.confusion_matrix(y_train, KNN_fitted.predict(X_train),labels=['Athlete','Fitness','Average','Obese'])\n",
    "conf = pd.DataFrame(conf, columns=['Athlete','Fitness','Average','Obese'], index = ['Athlete','Fitness','Average','Obese'])\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(class_weight= {'Obese':1,'Average':2,'Fitness':5,'Athlete':10},solver='liblinear')\n",
    "parameters = {'C':np.arange(.01,1.1,.02)}\n",
    "LR_mod_CV = GridSearchCV(LR, parameters,scoring=my_scorer)\n",
    "LR_fitted = LR_mod_CV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LR_fitted.best_params_)\n",
    "print(LR_fitted.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = metrics.confusion_matrix(y_train, LR_fitted.predict(X_train),labels=['Athlete','Fitness','Average','Obese'])\n",
    "conf = pd.DataFrame(conf, columns=['Athlete','Fitness','Average','Obese'], index = ['Athlete','Fitness','Average','Obese'])\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(class_weight= {'Obese':1,'Average':2,'Fitness':5,'Athlete':10})\n",
    "parameters = {'n_estimators':range(20,100,10),'min_samples_split':[2],'criterion':[\"gini\",\"entropy\"]}\n",
    "RF_mod_CV = GridSearchCV(RF,parameters,scoring=my_scorer)\n",
    "RF_fitted = RF_mod_CV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RF_fitted.best_params_)\n",
    "print(RF_fitted.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = metrics.confusion_matrix(y_train, RF_fitted.predict(X_train),labels=['Athlete','Fitness','Average','Obese'])\n",
    "conf = pd.DataFrame(conf, columns=['Athlete','Fitness','Average','Obese'], index = ['Athlete','Fitness','Average','Obese'])\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = GaussianNB()\n",
    "parameters = {'priors': [None,[0.25,0.25,0.25,0.25],[0.2,0.3,0.2,0.3]]}\n",
    "NB_mod_CV = GridSearchCV(NB,parameters,scoring=my_scorer)\n",
    "NB_fitted = NB_mod_CV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_fitted.best_estimator_.class_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NB_fitted.best_params_)\n",
    "print(NB_fitted.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = metrics.confusion_matrix(y_train, NB_fitted.predict(X_train),labels=['Athlete','Fitness','Average','Obese'])\n",
    "conf = pd.DataFrame(conf, columns=['Athlete','Fitness','Average','Obese'], index = ['Athlete','Fitness','Average','Obese'])\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch for personal scoring metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_points = np.array([[1,0.2,0.8,0],[0.2,1,0.4,0.6],[0.8,0.4,1,.2],[0,.6,.2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(['Athlete','Average','Fitness','Obese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin = lb.transform(y_test)\n",
    "y_test_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.matmul(y_test_bin,score_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interstep = np.matmul(y_test_bin,score_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interstep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = interstep*DTC_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_score = fin.sum()/len(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
